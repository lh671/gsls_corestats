---
title: "GSLS CS3- Single Contiunous Data"
author: "Liana Hardy"
date: '2020-05-11'
output: html_notebook
---

#### Aim:
To introduce R commands for analysing simple linear models

#### Goals

By the end of this practical participants should be able to perform the following statistical analyses:     
1. Simple Linear Regression   
2. Correlation     

For each of these, participants should be able to:
a. Perform the test in R     
b. Interpret the output     
c. Check the assumptions of the test     

#### Background

This practical focuses on the implementation of various statistical tests relating to simple linear regression and correlation.
Again, it does not focus on the underlying theory of the tests (although the demonstrators will be happy to answer any questions that you may have).

For each test there will be a section that

* explains the purpose of the test,    
* explains how to visualise the data     
* explains how to perform the test in R,    
* explains how to interpret the output and report the results, and       
* explains how to assess the assumptions required to perform the test.   

******

## 1 Datasets

This practical uses two internal datasets (i.e. these files come included with the R installation) and no extra files will be required.

## 2 Correlation Coefficients

### 2.1 Purpose and Aim
Correlation refers to the relationship of two variables (or data sets) to one another. In particular two datasets are said to be correlated if they are not independent from one another. Correlations can be useful because they can indicate whether a predictive relationship may exist. However just because two datasets are correlated does not mean that they are causally related.

### 2.2 Section Commands
New commands used in this section.

'cor'       *calculates a correlation matrix*      
'pairs'     *plots a matrix of scatter plots*

### 2.3 Data

We will use the internal __USArrests__ dataset for this example. This rather bleak dataset contains statistics in arrests per 100,000 residents for assault, murder and rape in each of the 50 US states in 1973, alongside the proportion of the population who lived in urban areas at that time. USArrests is an unstacked data frame with 50 observations of four variables: Murder, Assault, UrbanPop and Rape.


## 2.4 Pearson’s product moment correlation coefficient
Pearson’s r (as this quantity is also known) is a measure of the linear correlation between two variables. It has a value between -1 and +1, where +1 means a perfect positive correlation, -1 means a perfect negative correlation and 0 means no correlation at all.

### 2.4.1 Summarise and visualise
```{r}
pairs(USArrests, lower.panel=NULL)
```
* The first argument is a matrix or a data frame    
* The argument lower.panel tells R not to add the redundant reflected lower set of plots, below the diagonal     

From visual inspection of the scatterplots we can see that there appears to be a slight positive correlation between all pairs of variables, although this may be very weak in some case (Murder and UrbanPop for example).

#### 2.4.2 Implement the test

```{r}
cor(USArrests, method="pearson")
```

* The first argument is a matrix or a data frame    
* The argument method tells R which correlation coefficient to use     

The matrix gives the correlation coefficient between each pair of variables in the data frame. The matrix is symmetric (why?) and the diagonal values are all 1 (why?). The most correlated variables are Murder and Assault with an r value of 0.801. This appears to agree well with the set of scatterplots that we produced earlier.

### 2.5 Exercise
We will use the internal __state.x77__ dataset for this exercise. This rather more benign dataset contains information on more general properties of each US state, such as population (1975), per capita income (1974), illiteracy proportion (1970), life expectancy (1969), murder rate per 100,000 people, percentage of the population who are high-school graduates, average number of days where the minimum temperature is below freezing between 1931 and 1960, and the state area in square miles. state.x77 is a matrix with 50 rows and 8 columns, with column names: Population, Income, Illiteracy, Life.Exp, Murder, HS.Grad, Frost and Area.

Use the pairs command on the state.x77 dataset and visually identify 3 different pairs of variables that appear to be
1. the most positively correlated __murder & illiteracy; HS Grad & Income; HS Grad & Life expectancy__
2. the most negatively correlated __life expectancy & murder; illiteracy & frost; illiteracy & life expectancy__
3. not correlated at all __income & murder etc.__

Calculate Pearson’s r for all of the pairs of variables and see how well you were able to identify correlation visually.

##### Summarise and visualise the data 
```{r}
pairs(state.x77, lower.panel=NULL)
```
##### Implement the test
```{r}
cor(state.x77, method="pearson")
```

### 2.6 Spearman’s rank correlation coefficient

This test first calculates the rank of the numerical data (i.e. their position from smallest (or most negative) to the largest (or most positive)) in the two variables and then calculates Pearson’s product moment correlation coefficient using the ranks. As a consequence, this test is less sensitive to outliers in the distribution.

#### 2.6.1 implement the test
```{r}
cor(USArrests, method="spearman")
```  
* The first argument is a matrix or a data frame     
* The argument method tells R which correlation coefficient to use     

The matrix gives the correlation coefficient between each pair of variables in the data frame. Again, the matrix is symmetric, and the diagonal values are all 1 as expected. The values obtained are very similar to the correlation coefficients obtained using the Pearson test.

### 2.7 Exercise

##### Calculate Spearman’s correlation coefficient for the state.x77 dataset.

```{r}
cor(state.x77, method="spearman")
```

Which variable’s correlations are affected most by the use of the Spearman’s rank compared with Pearson’s r?
With reference to the scatterplot produced earlier, can you explain why this might this be?

## 3 Linear Regression
### 3.1 Purpose and Aim

Regression analysis not only tests for an association between two or more variables, but also allows one to investigate quantitatively the nature of any relationship which is present, and thus determine whether one variable may be used to predict values of another.
Simple linear regression essentially models the dependence of a scalar dependent variable (y) on an independent (or explanatory) variable (x) according to the relationship:

__y=β0 + β1*x__

where β0 is the value of the intercept and β0 is the slope of the fitted line. The aim of simple linear regression analysis to assess whether the coefficient of the slope, β1, is actually different from zero. If it is different from zero then we can say that x has a significant effect on y (since changing x leads to a predicted change in y), whereas if it isn’t significantly different from zero, then we say that there isn’t sufficient evidence of a relationship. Of course, in order to assess whether the slope is significantly different from zero we first need to calculate the values of β0 and β0.

### 3.2 Section Commands
No new commands used in this section.

### 3.3 Data and Hypotheses

We will perform a simple linear regression analysis on the two variables Murder and Assault from the USArrests dataset. We wish to determine whether the Assault variable is a significant predictor of the Murder variable. This means that we will need to find the coefficients β0 and β1 that best fit the following macabre equation:

__Murder =β0 + β1*Assault__

And then will be testing the following null and alternative hypotheses:
* H0: Assault is not a significant predictor of Murder β1 = 0.     
* H1: Assault is a significant predictor of Murder β1 ≠ 0.     

```{r}
plot(Murder~Assault, data=USArrests)
```
There appears to be a relatively strong positive relationship between these two variables and whilst there is a reasonable scatter of the points around any trendline, we would probably expect a significant result in this case.

### 3.5 implement the test
```{r}
lm.1<-lm(Murder~Assault, data=USArrests) # fit a straight line to your data
lm.1
```
* The first argument to lm is a formula saying that Murder depends on Assaults. As we have seen before, the syntax is generally dependent variable~independent variable.       
* The second argument specifies which dataset to use

The function lm returns a linear model (lm) object which is essentially a list containing everything necessary to understand and analyse a linear model. However, if we just type it in (as we have on the 2nd line) then it just prints to the screen the actual coefficients of the model i.e. the intercept and the slope of the line.

So here we have found that the line of best fit is given by:    
__Murder =0.63 + 0.042 Assault__

##### Assess whether the slope is significantly different from zero
```{r}
anova(lm.1)
```
Here, we again use the anova command to assess significance. This shouldn’t be too surprising at this stage if the introductory lectures made any sense. From a mathematical perspective, one-way and ANOVA and simple linear regression are exactly the same as each other and it makes sense that we should use the same command to analyse them in R.

### 3.6 Interpreting the output and reporting the results

This produces the following summary output for the linear regression.
This is exactly the same format as the table we saw for one-way ANOVA:

The 1st line just tells you the that this is an ANOVA test
The 2nd line tells you what the response variable is (in this case Murder)
The 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:
* The Df column contains the degrees of freedom values on each row, 1 and 48 (which we’ll need for the reporting)     
* The F value column contains the F statistic, 86.454 (which again we’ll need for reporting).    
* The p-value is 2.596e-12 and is the number directly under the Pr(>F) on the 4th line.    
* The other values in the table (in the Sum Sq and Mean Sq) column are used to calculate the F statistic itself and we don’t need to know these.  

Again, the p-value is what we’re most interested in here and shows us the probability of getting data such as ours if the null hypothesis were actually true and the slope of the line were actually zero.
Since the p-value is excruciatingly tiny we can reject our null hypothesis and state that:

__A simple linear regression showed that the assault rate in US states was a significant predictor of the number of murders (F=86.45, df=1,48, p=2.59x10-12).__

#### 3.6.1 Plotting the regression line
```{r}
plot(Murder~Assault, data=USArrests) #The first command creates a scatterplot of the data
abline(lm.1,col="red") # The second command uses the results of the linear model fitting (the object lm.1) to add the line of best fit to the plot (and colour it red).
```
### 3.7 Assumptions

In order for a linear regression analysis to be valid 4 key assumptions need to be met:
1. The data must be linear (it is entirely possible to calculate a straight line through data that is not straight - it doesn’t mean that you should though!)       
2. The residuals must be normally distributed       
3. The residuals must not be correlated with their fitted values    
4. The fit should not depend overly much on a single point (no point should have high leverage).    

*Whether these assumptions are met can easily be checked visually by producing four key diagnostic plots.*

#### 3.7.1.1 R screen output

```{r}
par(mfrow=c(2,2))
plot(lm.1)
```
* The first command changes the plotting parameters and splits the graphics window into 2 rows and 2 columns (you won’t notice anything whilst you run it).
* The second command produces 4 plots in the graphics window  

The top left graph plots the residuals against the fitted values. If the data are best explained by a straight line then there should be a uniform distribution of points above and below the horizontal grey dotted line (and if there are sufficient points then the red line, which is a moving average, should be on top of the grey dotted line). This plot is pretty good.

The top right graph allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal dotted line. This isn’t too bad but there is some slight snaking towards the upper end and Georgia appears to be an outlier here.

The bottom left graph allows us to investigate whether there is any correlation between the residuals and the fitted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.

The last graph tests to see if any one point has an unnecessarily large effect on the fit. The important aspect here is to see if any points lie beyond the red dashed contour line in the top right corner of this plot. If not, then no point has undue influence. This plot is good.

*Formally, if there is any concern after looking at the diagnostic plots, then a linear regression is not valid. However, disappointingly, very few people ever check whether the linear regression assumptions have been met before quoting the results.*

### 3.8 Exercise

#### Calculate two simple linear regressions using the state.x77 dataset, firstly for the variable Life.Exp on the variable Murder and secondly for the variable HS.Grad on Frost.

The data is currently only a matrix and so before you can proceed you will need to store it as a data frame using the data.frame() function.

#### Create dataframe
```{r}
states<-data.frame(state.x77)
states
```

#### Plot
```{r}
plot(Life.Exp~Murder, data=states)
plot(HS.Grad~Frost, data=states)
```
#### Fit a straight line to data, calculate
```{r}
lm.murder<-lm(Life.Exp~Murder, data=states) # fit a straight line to your data
lm.murder

lm.frost<-lm(HS.Grad~Frost, data=states) # fit a straight line to your data
lm.frost
```

#### Perform ANOVA to determine if slope is different from zero
```{r}
anova(lm.murder)
anova(lm.frost)
```

#### Scatter plot with line of best fit 
```{r}
plot(Life.Exp~Murder, data=states) 
abline(lm.murder,col="green") 

plot(HS.Grad~Frost, data=states) 
abline(lm.frost,col="purple") 
```
#### Diagnostic plots
```{r}
par(mfrow=c(2,2))
plot(lm.murder)

par(mfrow=c(1,1))
plot(lm.frost)
```

Do the following in both cases:   
1. Find the value of the slope and intercept coefficients for both regressions        
2. Determine if the slope is significantly different from zero (i.e. is there a relationship between the two variables)       
3. Produce a scatterplot of the data with the line of best fit superimposed on top.      
4. Produce diagnostic plots and discuss with your neighbour whether you should have carried out a simple linear regression in each case   
- Life.Exp~Murder, diagnostic plots show assumptions are met. Linear regression is fine in this case.
- HS.Grad~Frost, simple linear regression not appropriate here. 

